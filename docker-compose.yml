version: '3.7'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    webserver:
        image: puckel/docker-airflow:1.10.9
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
          - ./docker/airflow/dags:/usr/local/airflow/dags
        ports:
            - "8090:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    spark-master:
      image: bde2020/spark-master:3.0.1-hadoop3.2
      ports:
        - "8080:8080"
        - "7077:7077"
      environment:
        - INIT_DAEMON_STEP=setup_spark
        - "constraint:node==spark-master"
      volumes:
        - ./twitter:/spark/twitter
  
    spark-worker:
      image: bde2020/spark-worker:3.0.1-hadoop3.2
      depends_on:
        - spark-master
      ports:
        - "8081:8081"
      volumes:
        - ./twitter:/spark/twitter
      environment:
        - "SPARK_MASTER=spark://spark-master:7077"
        - "constraint:node==spark-worker"
        
    hadoop:
      image: sequenceiq/hadoop-docker:2.7.0
      ports:
        - "50010:50010"
        - "50020:50020"
        - "50070:50070"
        - "50075:50075"
        - "50090:50090"
        - "8089:8088"
        - "8020:8020"
        - "9000:9000"